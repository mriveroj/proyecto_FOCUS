### EJEMPLO 3: ARRANGE
arrange(storms, pressure)
arrange(storms, wind, pressure, category)
arrange(storms, wind, pressure, year)
arrange(storms, desc(year))
arrange(storms, name)
name <- arrange(storms, name)
View(name)
arrange(storms, desc(year), pressure, name)
### EJEMPLO 3: GROUP_BY, MUTATE & SUMMARISE
group_by(storms, name)
### EJEMPLO 3: GROUP_BY, MUTATE & SUMMARISE
group_by(storms, status)
### EJEMPLO 3: GROUP_BY, MUTATE & SUMMARISE
group_by(storms, name)
group_by(storms, name, status)
storms %>% group_by(name, status) %>% mutate("Date"= paste(year, month, day, hour))
storms %>% group_by(name, status) %>% mutate("Date"= paste(year,"/",month,"/",day))
storms %>% group_by(name, status) %>% mutate("Date"= paste0(year,"-",month,"-",day))
storms <- storms %>% group_by(name, status) %>% mutate("Date"= paste0(year,"-",month,"-",day))
storms$Date <-  lubridate::ymd(storms$Date)
str(storms)
str(storms)
View(storms)
class(storms$Date)
glimpse(storms)
storms %>% group_by(year, name) %>% summarise(mean_wind=mean(wind), mean_pressure = mean(pressure))
View(storms)
storms %>% mutate(pressure = pressure*100)
storms %>% mutate(pressure2 = pressure*100)
storms <- storms %>% group_by(name, status) %>% mutate("Date"= paste0(year,"-",month,"-",day))
View(storms)
# Importamos y miramos los datos
storms <- dplyr::storms
storms
str(storms)
storms <- storms %>%  mutate("Date"= paste0(year,"-",month,"-",day))
View(storms)
storms$Date <-  lubridate::ymd(storms$Date)
storms %>% mutate(pressure_pascal = pressure*100)
storms <- select(storms, -c(ts_diameter, hu_diameter))
### EJEMPLO 3: GROUP_BY, MUTATE & SUMMARISE
group_by(storms, name)
group_by(storms, name, status)
storms <- storms %>%  mutate("Date"= paste0(year,"-",month,"-",day))
storms$Date <-  lubridate::ymd(storms$Date)
storms %>% mutate(pressure_pascal = pressure*100)
storms %>% group_by(year, name) %>% summarise(mean_wind=mean(wind), mean_pressure = mean(pressure))
str(storms)
storms %>% group_by(year, status) %>% summarise(Frequency=n())
storms %>% mutate(pressure_pascal = pressure*100)
## Organizamos toda la base de datos
storms <- dplyr::storms
View(storms)
storms <- storms %>% mutate(pressure_pascal = pressure*100)
View(storms)
View(storms)
## Organizamos toda la base de datos
storms <- dplyr::storms
storms <- storms %>%  mutate("Date"= paste0(year,"-",month,"-",day))
storms <- storms %>% mutate(pressure_pascal = pressure*100)
View(storms)
## Organizamos toda la base de datos
storms <- dplyr::storms
storms <- storms %>%  mutate("Date"= paste0(year,"-",month,"-",day))
storms <- storms %>% mutate(pressure = pressure*100)
View(storms)
storms <- storms %>% select(-c(year, month, day, hour, ts_diameter_ hu_diameter))
storms <- storms %>% select(-c(year, month, day, hour, ts_diameter, hu_diameter))
View(storms)
## Organizamos toda la base de datos
storms <- dplyr::storms
storms <- storms %>%  mutate("date"= paste0(year,"-",month,"-",day))
storms <- storms %>% mutate(pressure = pressure*100)
storms <- storms %>% select(-c(year, month, day, hour, ts_diameter, hu_diameter))
storms <- storms %>% select(date, name, status, everything())
storms <- storms %>% select(date, name, status, category, everything())
storms_init <- dplyr::storms
storms_init
storms
## Organizamos toda la base de datos
storms <- dplyr::storms
storms_init <- dplyr::storms
storms <- storms %>%  mutate("date"= paste0(year,"-",month,"-",day))
storms <- storms %>% mutate(pressure = pressure*100)
storms <- storms %>% mutate(wind = wind*1.852)
storms <- storms %>% select(-c(year, month, day, hour, ts_diameter, hu_diameter))
storms <- storms %>% select(date, name, status, category, everything())
storms_init
storms
View(storms)
filter(storms, category == 5)
install.packages("RODBC")
library(RODBC)
# conectar trabla
cn  <- odbcDriverConnect(connection="Driver={SQL Server};server=LAPTOP-BIT1OKR8;database=Analytics;trusted_connection=yes;")
library(sqldf)
library(datos)
paises <- datos::paises
# Seleccionamos toda la tabla
sqldf("SELECT * FROM paises")
# Seleccionamos las 2 primeras columnas
sqldf("SELECT pais, continente FROM paises")
# Seleccionamos un conjunto de datos, no todos
sqldf("SELECT * FROM paises LIMIT 5")
# Sumamos 2 columnas para sumar
sqldf("SELECT sum(anio, esperanza_de_vida) FROM paises LIMIT 5")
# Sumamos 2 columnas para sumar
sqldf("SELECT sum(anio) FROM paises")
sqldf("SELECT sum(anio) AS 'Sum_Anios' FROM paises")
sqldf("SELECT sum(Anios) AS 'Sum_Anios' , avg(esperanza_de_vida) AS 'Mean_vida' FROM paises")
sqldf("SELECT sum(anio) AS 'Sum_Anios' , avg(esperanza_de_vida) AS 'Mean_vida' FROM paises")
# Contamos numero de continentes
sqldf("SELECT count(Continente) AS 'Cantidad_Asia' FROM paises WHERE Continente == 'Asia'")
# Utilizando el group_by
sqldf("SELECT Continente AS 'Nombre_continentes' FROM paises GROUP BY Continente")
# Utilizando el group_by y desc
sqldf("SELECT Continente AS 'Nombre_continentes' FROM paises GROUP BY Continente DESC" )
# Utilizando el group_by y desc
sqldf("SELECT Continente AS 'Nombre_continentes' FROM paises GROUP BY Continente ASC" )
# Utilizando el group_by y desc
sqldf("SELECT Continente AS 'Nombre_continentes' FROM paises ASC GROUP BY Continente " )
# Utilizando el group_by y desc
sqldf("SELECT Continente AS 'Nombre_continentes' FROM paises DESC GROUP BY Continente " )
# Utilizando el group_by y desc
sqldf("SELECT Continente AS 'Nombre_continentes' FROM paises GROUP BY Continente " )
sqldf("SELECT upper(Pais) AS 'PAIS', lower(Continente) AS 'Continente' FROM paises")
sqldf("SELECT upper(Pais) AS 'PAIS', lower(Continente) AS 'Continente' FROM paises LIMIT 5")
sqldf("SELECT Continente, pais, anio, poblacion FROM paises WHERE anio<20 LIMIT 20")
sqldf("SELECT Continente, pais, anio, poblacion FROM paises WHERE anio<20 LIMIT 20")
sqldf("SELECT Continente, pais, Anio, poblacion FROM paises WHERE anio<20 LIMIT 20")
sqldf("SELECT Continente, pais, Anio, poblacion FROM paises WHERE Anio<20 LIMIT 20")
sqldf("SELECT Continente, pais, anio, poblacion FROM paises WHERE anio<20 LIMIT 20")
sqldf("SELECT Continente, pais, anio, poblacion FROM paises WHERE anio<2015 LIMIT 20")
# Seleccionar datos que empiecen con algun elemento
sqldf("SELECT * FROM paises WHERE pais LIKE '%A' LIMIT 5")
# Seleccionar paises que contenga la letra A
sqldf("SELECT * FROM paises WHERE pais LIKE '%A%' LIMIT 5")
# Seleccionar paises que contenga la letra A
sqldf("SELECT * FROM paises WHERE pais LIKE '%O%' LIMIT 5")
seq(0,3)
?lines
#Supply Chain Resilience Model
# University of South Florida 2018
library(ggplot2)
# Creamos el data frame con las 100 iteraciones
runs<-100
set.seed(1)
insdat <- data.frame("Intensidad Falla"=sample(3:30,runs, replace=TRUE),
"Cantidad Q"=sample(3:15, runs, replace=TRUE),
"Lead Time"=sample(1:21, runs, replace=TRUE),
"Nivel de Servicio"=runif(runs,min=0.5,max=0.999)
)
# Guardamos las configuraciones de la cadena de suministro
#write.csv(insdat, file = "InstancesSC.csv")
# Creamos matrices vacias para guardar los resultados
Intensidad<-insdat[,1]
mat_impacto_NivelDeServicio<-matrix(,ncol=1,nrow=runs)
mat_tiempo_impacto<-matrix(,ncol=1,nrow=runs)
PerformanceLossMat <- matrix(,ncol=1,nrow=runs) # Esta es la que nos interesa
for (z in 1:runs){ # Este ciclo es el número de instancias, en este caso 100
sum_impacto=0
sum_tiempo=0
PerformanceLoss <- 0
for (g in 1:30) { # Este ciclo son 30 répicas para cada instancia
#Configuracion numero de elementos en nivel 1 y 2 de la cadena de suministro
n1<-1
n2<-1
# Parámetros
demanda<-100
desviacion_demanda<-10
lead.time<-insdat[z,3]
zsl<-qnorm(insdat[z,4],0,1)
Inventario_seguridad<-ceiling(demanda*(lead.time+0.5)+zsl*desviacion_demanda*sqrt(lead.time)) # Al ejecutar S sale que corresponde a un Lead Time, pero pensé que ya el Lead Time se tiene en cuenta con la variable l  definida 2 lineas antes
Q<-demanda*insdat[z,2]
delay1<-0
# Parameters to Measure X and T
switch_recovery_start=0 # Switch recovery start
switch_recovery_end=0 # Switch recovery end
tiempo_fin_disrupcion=0
tiempo_inicio_disrupcion=0
maxx=0 # El máximo de las X (de las caidas en nivel de servicio)
pt <- 0
sendDate <- 0
promisedLT <- 0
actualLT <- 0
On_Hand_inventory<-Inventario_seguridad+Q
intransit<-0
order<-matrix(0,nrow=300,ncol=2)
matdat<-matrix(0,nrow=365,ncol=8)
corder<-0
inventario_posicionado<-On_Hand_inventory+intransit
#Generate Failure
duracion_disrupcion<-insdat[z,1] # Duración de la falla
disrupcion_inicio<-ceiling(runif(n =1,min =15, max=200)) # Tiempo aleatorio de inicio de la falla
disrupcion_fin<-disrupcion_inicio+duracion_disrupcion # Tiempo de Finalización de la falla
# Hay 1 falla por cada corrida (instancia)
#Functions
# Order Function
for (i in 1:365) {
d<-rnorm(n = 1,mean=100,sd=10) # Demanda de clientes finales
if (i>=disrupcion_inicio && i<=disrupcion_fin) { # Si se encuentra dentro del tiempo de falla, se activa swith y no se hacen despachos
switch_disrupcion=1
} else {
switch_disrupcion=0
}
# Servir clientes finales
if (On_Hand_inventory>0){
dispatch<-min(d,On_Hand_inventory)
On_Hand_inventory<-On_Hand_inventory - dispatch
} else {
dispatch<-0
}
lsale <- d-dispatch # Lost sales (ventas perdidas)
inventario_posicionado <- On_Hand_inventory+intransit
servicel <- dispatch/d # Nivel de servicio
# Check Inventories Retailers
if (inventario_posicionado <= Inventario_seguridad && switch_disrupcion==0){
corder<-corder+1
order[corder,1]<-Q
order[corder,2]<-i+lead.time # Tiempo en el cual va a llegar el pedido (hoy = 1 + lead time = l)
intransit<-intransit+order[corder,1]
matdat[i,5]<-order[corder,1]
}else {
matdat[i,5]<-0
}
inventario_posicionado <- On_Hand_inventory + intransit
# Para los pedidos que se enviaron durante la disrupcion
if (corder > 0 && switch_disrupcion==1){
sendDate <- order[corder,2] - lead.time
promisedLT <- insdat[z,3]
}
# Añadir la orden al inventario On-Hand una vez que llegue
# Este ciclo de acá representa la recepción de mercancías en el retailer
if (corder>0){
for (k in 1:corder) {
if (order[k,2]<= i) {
On_Hand_inventory<- On_Hand_inventory + order[k,1]
intransit<- intransit-order[k,1]
order<- order[-k,]
corder<- corder-1
}
}
}
# Para los pedidos que se enviaron durante la disrupcion
if (corder>=0 && switch_disrupcion==1 && sendDate>0){
actualLT <- i - sendDate
}
# Resilience Response Variables
if (i>=disrupcion_inicio && switch_recovery_start==0 && servicel<=0.95 ) {
switch_recovery_start=1
tiempo_inicio_disrupcion=i
}
if (i>=disrupcion_fin && switch_recovery_end==0 && switch_recovery_start==1 && servicel>=0.95 ) {
switch_recovery_end=1
tiempo_fin_disrupcion=i
}
if (i>=disrupcion_inicio){
maxx<-max(maxx,(1-servicel))  # Se toma el máximo de las disrupciones
}
# Performance Loss Muñoz & Dunbar (2015)
if(i>= disrupcion_inicio && switch_recovery_start==1 && switch_recovery_end==0){
pt <- pt + (1-servicel)
}
# Record Variables
Tr=tiempo_fin_disrupcion-tiempo_inicio_disrupcion # Recovery Muñoz & Dunbar (2015)
X=maxx # Impacto Muñoz & Dunbar (2015)
matdat[i,1]<-d
matdat[i,2]<-inventario_posicionado
matdat[i,3]<-On_Hand_inventory
matdat[i,4]<-intransit
matdat[i,6]<-lsale
matdat[i,7]<-servicel
matdat[i,8]<-Inventario_seguridad
colnames(matdat)<-c("Demand","Inv. Pos","On-Hand","In-transit","Orders","Loss Sales","Service Level", "Safety Stock")
}
sum_impacto<-(sum_impacto+X) # Impact Muñoz & Dunbar (2015)
sum_tiempo<-(sum_tiempo+Tr) # Recovery Muñoz & Dunbar (2015)
# Performance loss Muñoz & Dunbar (2015)
PerformanceLoss <- PerformanceLoss + ((tiempo_fin_disrupcion-tiempo_inicio_disrupcion)*X-pt)
}
mat_impacto_NivelDeServicio[z]<-sum_impacto/g
mat_tiempo_impacto[z]<-sum_tiempo/g
PerformanceLossMat[z] <- PerformanceLoss/g
}
#write.csv(matdat, file="mydata.csv")
invPos <- data.frame("y"=matdat[, 3], "x"=seq(1,365))
servLevel <- data.frame("y"=matdat[, 7], "x"=seq(1,365))
##### GRAFICAS MÉTRICAS BÁSICAS
# Inventory positioning plot
ggplot(invPos, aes(x=x, y=y)) + geom_line(color="seagreen3") +
theme_minimal() +
theme(plot.title = element_text(face="bold", hjust = 0.5)) +
ggtitle("Inventory Positioning") +
ylab("Number of units") +
xlab("Days")
# Service level plot
ggplot(servLevel, aes(x=x, y=y)) + geom_line(color="slategray") +
theme_minimal() +
theme(plot.title = element_text(face="bold", hjust = 0.5)) +
ggtitle("Service level") +
ylab("Service level") +
xlab("Days") +
scale_y_continuous(limits=c(0,1))
# Impact metric:  Muñoz & Dunbar (2015)
impact <- data.frame("x"=Intensidad, "y"=mat_impacto_NivelDeServicio)
ggplot(impact, aes(x=x, y=y)) + geom_point(color="forestgreen") +
theme_minimal() +
xlab("Disruption intensity") +
ylab("Service level loss") +
ggtitle("Impact metric") +
theme(plot.title = element_text(face="bold", hjust = 0.5)) +
ylim(c(0,1))
# Recovery metric:  Muñoz & Dunbar (2015)
recovery <- data.frame("x"=Intensidad, "y"=mat_tiempo_impacto)
ggplot(recovery, aes(x=x, y=y)) + geom_point(color="purple") +
theme_minimal() +
xlab("Disruption intensity") +
ylab("Time to recover (days)") +
ggtitle("Recovery metric") +
theme(plot.title = element_text(face="bold", hjust = 0.5))
#### Calculamos métricas de la literatura
# Metrica Romero, D. (2020)
R1<-(Intensidad^2)/(mat_impacto_NivelDeServicio*mat_tiempo_impacto)
R1<-R1/max(R1)
# Metrica Zobel, C.W. (2011)
Tmax=32
R2<-1-(mat_impacto_NivelDeServicio*mat_tiempo_impacto)/Tmax
RM1 <- data.frame("y"=R1, "x"=Intensidad)
RM2 <- data.frame("y"=R2, "x"=Intensidad)
RM4 <- data.frame("y"=PerformanceLossMat, "x"=Intensidad)
##### GRAFICAS METRICSA TEORIA
# Resilience metric 1: Romero, D. (2020)
ggplot(RM1, aes(x=x, y=y)) + geom_point(color="dodgerblue") +
theme_minimal() +
xlab("Disruption intensity") +
ylab("Service level loss") +
ggtitle("Resilience metric1") +
theme(plot.title = element_text(face="bold", hjust = 0.5))
# Resilience metric 2: Zobel, C.W. (2011)
ggplot(RM2, aes(x=x, y=y)) + geom_point(color="coral3") +
theme_minimal() +
xlab("Disruption intensity") +
ylab("Resilience metric 2") +
ggtitle("Resilience metric 2") +
theme(plot.title = element_text(face="bold", hjust = 0.5))
#  ESTA ES: Performance loss: Muñoz & Dunbar (2015)
ggplot(RM4, aes(x=x, y=y)) + geom_point(color="red2") +
theme_minimal() +
xlab("Disruption intensity") +
ylab("Performance loss") +
ggtitle("Performance Loss metric") +
theme(plot.title = element_text(face="bold", hjust = 0.5))
# Statistical Analysis
fitt<- lm(mat_tiempo_impacto~insdat[,1]+insdat[,2]+insdat[,3]+insdat[,4])
summary(fitt)
fitx<-lm(mat_impacto_NivelDeServicio~insdat[,1]+insdat[,2]+insdat[,3]+insdat[,4])
summary(fitx)
matriz_final <- data.frame(insdat, PerformanceLossMat)
fit2<-lm(PerformanceLossMat~Cantidad.Q+Lead.Time+Nivel.de.Servicio, data=matriz_final)
summary(fit2)
i
z
g
citation(caret)
library(caret)
citation(caret)
citation("caret")
setwd("~/proyecto_FOCUS/Grupo Web Scraping")
library(RColorBrewer)
library(plyr)
library(rtweet)
library(wordcloud)
library(stringr)
library(tm)
library(SnowballC)
library(reshape2)
library(dplyr)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
comentarios <- read.csv(file="Comentarios.csv",sep=";", na.strings=c("","NA"))
comentarios <- comentarios[-1,]
comentarios_tripadvisor <- read.csv("comentarios_separados.csv") %>% select(-c(1,2))
i=2
mat_comentarios <- c()
for(i in 1:dim(comentarios_tripadvisor)[2]){
com_temp <- comentarios_tripadvisor[, i]
com_temp <- com_temp[complete.cases(com_temp)]
mat_comentarios <- c(mat_comentarios, com_temp)
}
for(i in 1:dim(comentarios)[2]){
com_temp2 <- comentarios[, i]
com_temp2 <- com_temp2[complete.cases(com_temp2)]
mat_comentarios <- c(mat_comentarios, com_temp2)
}
mat_comentarios <- as.data.frame(mat_comentarios)
mat_comentarios <- mat_comentarios[!duplicated(mat_comentarios), ]
rm(com_temp, com_temp2)
corpus_restaurantes <- Corpus(VectorSource(mat_comentarios))
#inspect(corpus_restaurantes[1:3])
corpus_restaurantes <-tm_map(corpus_restaurantes, str_replace_all, pattern="[[:punct:]]", replacement = " ")
corpus_restaurantes <-tm_map(corpus_restaurantes, str_replace_all, pattern="[^a-zA-Z0-9 ]", replacement = "")
corpus_restaurantes <-tm_map(corpus_restaurantes, str_replace_all, pattern="^\\s+|\\s+$", replacement = "")
corpus_restaurantes <-tm_map(corpus_restaurantes, tolower)
corpus_restaurantes <-tm_map(corpus_restaurantes, removeNumbers)
corpus_restaurantes <-tm_map(corpus_restaurantes, stemDocument)
corpus_restaurantes<-tm_map(corpus_restaurantes,stripWhitespace)
corpus_restaurantes <- tm_map(corpus_restaurantes, removeWords, stopwords(kind = "es"))
inspect(corpus_restaurantes[1:3])
restaurantes_tdm <- TermDocumentMatrix(corpus_restaurantes, control = list(stopwords = TRUE)) %>% as.matrix()
restaurantes_dtm <- DocumentTermMatrix(corpus_restaurantes, control = list(minWordLength = 1, stopwords = TRUE))
inspect(restaurantes_dtm) # Permite ver cuantas veces aparecen las palabras en el documento
restaurantes_tdm<-as.matrix(restaurantes_tdm)
restaurantes_tdm[1:10,1:20]
head(findFreqTerms(restaurantes_dtm, lowfreq=20), 40)
w<-rowSums(restaurantes_tdm)
w<-subset(w,w>=500)
barplot(w,
las=2,
col=rainbow(50))
w <- as.data.frame(w)
w = data.frame("palabras"=rownames(w), "frecuencia"=w$w)
ggplot(w, aes(x=palabras, y=frecuencia)) + geom_bar()
ggplot(w, aes(x=palabras, y=frecuencia)) + geom_bar(stat="identity")
ggplot(w, aes(x=reorder(palabras, -frecuencia), y=frecuencia)) + geom_bar(stat="identity")
ggplot(w, aes(x=reorder(palabras, -frecuencia), y=frecuencia)) + geom_bar(stat="identity", fill="dodgerblue2", alpha=0.7) +
theme_bw() +
coord_flip()
ggplot(w, aes(x=reorder(palabras, frecuencia), y=frecuencia)) + geom_bar(stat="identity", fill="dodgerblue2", alpha=0.7) +
theme_bw() +
coord_flip()
ggplot(w, aes(x=reorder(palabras, frecuencia), y=frecuencia)) + geom_bar(stat="identity", fill="dodgerblue2", alpha=0.7) +
theme_bw() +
coord_flip() +
labs(x="Palabras más frecuentes", y="Frecuencia")
w = data.frame("palabras"=rownames(w), "frecuencia"=w$w) %>% head(10)
w<-rowSums(restaurantes_tdm)
w<-subset(w,w>=500)
barplot(w,
las=2,
col=rainbow(50))
w <- as.data.frame(w)
w = data.frame("palabras"=rownames(w), "frecuencia"=w$w) %>% head(10)
ggplot(w, aes(x=reorder(palabras, frecuencia), y=frecuencia)) + geom_bar(stat="identity", fill="dodgerblue2", alpha=0.7) +
theme_bw() +
coord_flip() +
labs(x="Palabras más frecuentes", y="Frecuencia")
w<-rowSums(restaurantes_tdm)
w
w<-rowSums(restaurantes_tdm)
#w<-subset(w,w>=500)
w <- as.data.frame(w)
w = data.frame("palabras"=rownames(w), "frecuencia"=w$w) %>% head(10)
ggplot(w, aes(x=reorder(palabras, frecuencia), y=frecuencia)) + geom_bar(stat="identity", fill="dodgerblue2", alpha=0.7) +
theme_bw() +
coord_flip() +
labs(x="Palabras más frecuentes", y="Frecuencia")
View(w)
w = data.frame("palabras"=rownames(w), "frecuencia"=w$w) %>% arrange(n) %>%  head(10)
w<-rowSums(restaurantes_tdm)
#w<-subset(w,w>=500)
w <- as.data.frame(w)
w = data.frame("palabras"=rownames(w), "frecuencia"=w$w) %>% arrange(n) %>%  head(10)
#w<-subset(w,w>=500)
w <- as.data.frame(w)
View(w)
w = data.frame("palabras"=rownames(w), "frecuencia"=w$w) %>% arrange(w) %>%  head(10)
View(w)
w<-rowSums(restaurantes_tdm)
#w<-subset(w,w>=500)
w <- as.data.frame(w)
w = data.frame("palabras"=rownames(w), "frecuencia"=w$w) %>% arrange(-w) %>%  head(10)
View(w)
ggplot(w, aes(x=reorder(palabras, frecuencia), y=frecuencia)) + geom_bar(stat="identity", fill="dodgerblue2", alpha=0.7) +
theme_bw() +
coord_flip() +
labs(x="Palabras más frecuentes", y="Frecuencia")
View(w)
library(wordcloud2)
wordcloud2(w,
size = 0.5,
shape = "star",
rotateRatio = 0.5,
minSize = 1)
comentarios<-iconv(comentarios,to="UTF-8")
comentarios
?iconv
mat_comentarios
mat_comentarios<-iconv(mat_comentarios,to="UTF-8")
mat_comentarios[1:3]
mat_comentarios<-iconv(mat_comentarios,to="latin1")
mat_comentarios[1:3]
mat_comentarios<-iconv(mat_comentarios,to="")
mat_comentarios[1:3]
mat_comentarios<-iconv(mat_comentarios,from = "", to="UTF-8")
mat_comentarios[1:3]
mat_comentarios<-iconv(mat_comentarios,from = "", to="latin1")
mat_comentarios[1:3]
s<-get_nrc_sentiment(corpus_restaurantes,language = "spanish")
comentarios <- read.csv(file="Comentarios.csv",sep=";", na.strings=c("","NA"))
comentarios <- comentarios[-1,]
View(comentarios)
comentarios_tripadvisor <- read.csv("comentarios_separados.csv") %>% select(-c(1,2))
View(comentarios_tripadvisor)
comentarios_trip2 <- iconv(comentarios_tripadvisor, to="UTF-8")
comentarios_trip2
comentarios_trip2 <- iconv(comentarios_tripadvisor, to="latin1")
datos <- readRDS("restaurant_trip_advisor.rds")
library(tidyverse)
dat<- data.frame(id=datos$id, comentarios= datos$Comentarios)
